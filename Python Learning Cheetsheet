# Python Learning Cheetsheet
# Dictionary
  .keys()
  .values()
  .items()
# in Python, there is no native array data structure. So, we use Python lists instead of an array.
# A Pandas Series is a one-dimensional array of indexed data.

# Example: flatten the lists in dicts and create a new dict
  def most_common(menu):
    newlist = []
    for value in menu.values():
      newlist += value
    newdict = {}
    for item in newlist:
      if item not in newdict.keys():
        newdict[item] = 1
      else:
        newdict[item] += 1
    name = None 
    longest = -1
    for item in newdict.keys():
      if newdict[item] > longest:
        longest = newdict[item]
        name = item
    return "%s:%s" % (name, longest)
    
# PANDAS
## Basic Data Manipulation

0. Create a dataframe --> right = DataFrame({'key1': ['foo', 'foo', 'bar', 'bar'], 'key2': ['one', 'one', 'one', 'two'], 'rval': [4, 5, 6, 7]})
1. Select a specific row --> df.ix[1] or Select several rows --> df[:3]; Select a column --> df['colname'] or Select multiple colums --> df[['colname1','colname2']][14:18]
2. Random sample from dataframe: df.sample(n=5)
3. Sorting: df.sort_values(by='colname', ascending=False)[:10]
4. Subsetting
data[data.density > 100]

    .loc[] --> the loc attribute allows indexing and slicing that always references the explicit index
    .iloc[] --> the iloc attribute allows indexing and slicing that always references the implicit Python-style index

5. Partially matching text with .str.contains()
6. Groupby
   df.groupby(['LINCC_last_step','LINCC_last_vist_date'])['user_id'].count()
   .unstack()
7. Pivot Tables
   DataFrame.pivot_table(data, values=None, index=None, columns=None,
                      aggfunc='mean', fill_value=None, margins=False,
                      dropna=True, margins_name='All')
                      
   You can define how values are grouped by:
   index= (“Rows” in Excel)
   columns=
   We define which values are summarized by: values= the name of the column of values to be aggregated in the ultimate table, then grouped by the Index and Columns and aggregated according to the Aggregation Function
   We define how values are summarized by: aggfunc= (Aggregation Function) how rows are summarized, such as sum, mean, or count
   e.g. 
   flights_by_carrier = df.pivot_table(index='flight_date', columns='unique_carrier', values='flight_num', aggfunc='count')

8. Common functions:  
   (1) df.country_code.fillna(df.user_country, inplace=True)
   (2) df=df.drop_duplicates(['user_id'], keep='last')
   (3) df=pd.merge(df1,df2,how='left',left_on='account_id',right_on='ads_account_id',indicator=True)
       df0=df[df['_merge']=='left_only']
   (4) df0=df.dropna(subset=['LINCC_last_step', 'BTC_last_page', 'Whitelist_type'], how='all')
   (5) replace value --> data.loc[data.company_name =='The Imprint Doctor','twitter_handle_lower'] = "theimprintdr"
   (6) pd.concat
       pd.append

9. Functions
    Example:
    def filter_desktop_mobile(platform):
        if platform in mobile:
            return 'Mobile'
        elif platform == 'Desktop':
            return 'Desktop'
        else:
            return 'Not Known'

10. The .apply() method allows you to apply a function to a column of a DataFrame. 
e.g. data['platform'].apply(filter_desktop_mobile)
     data['delayed'] = data['arr_delay'].apply(lambda x: x > 0)

11. Resetting Index
    data2 = data1.copy()
    data2 = data2.reset_index()
    del data2['index']


## Modeling Steps
1. Load data:
   pd.read_csv('mattermark_adv_handle_df.csv',encoding='utf-8')
or pd.read_csv('.csv', thousands=",")
2. Data Quality Check:
   (0) data.describe()
   (1) Duplicates
        df.drop_duplicates(subset='company_id')
   (2) Missing Data
        df[pd.isnull(df.id)==False]
        data['employees_flag'] = data['employees'].apply(update_null)
        data.employees.fillna(0,inplace=True)
        
         # Functions
            ### Remove Columns
                def remove_col_from_df(df,col_list):
                    for col in col_list:
                        try:
                            df.drop(col,axis=1,inplace=True)
                        except:
                            print str(col) + ' not present to drop'
                    return df

            ### Change Null as 0 else 1
                def update_null(c):
                    if pd.isnull(c) == False:
                        return 1
                    else:
                        return 0

            ### Check NAs percentage in columns
                def get_NA_ratio (df, col_list):
                    for col in col_list:
                        try:
                            print str(col) + ": " + str(df[col].isnull().sum().astype(float)/len(df[col]))
                        except:
                            print str(col) + ' cannot get NAs'
    (3) Data Type + Encoding
        data['employees'] = data['employees'].astype(int)
        data.loc[data["has_mobile"] == "Yes", "has_mobile"] = 1
    
    (4) Scatter Plot + Histogram
        import matplotlib as mpl
        import matplotlib.pyplot as plt 
        import numpy

        #data['cached_growth_score_log'] = np.log(data['cached_growth_score'].astype('float'))
        #data['custom_score_log'] = np.log(data['custom_score'].astype('float'))
        plt.scatter(y=data.mindshare_score,x=data.mattermark_score)
        plt.show()
        
        bins = numpy.linspace(0, 2000, 10)
        plt.hist(data.cached_growth_score, bins, alpha=0.5)
        plt.show()
        
3. Model Building
   (1) Create a flag/label:
    pre_training_df['HighSpenderFlag'] = np.where(pre_training_df['total_spend'] >= 500,1,0)
   
   (2) Create training/test set:
    X = model_training_df.drop('HighSpenderFlag',axis=1)
    y = model_training_df['HighSpenderFlag']
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=0)
   
   (3) Perform feature selection and remove cols based on results:
    selector = SelectKBest(f_classif, k='all')
    selector.fit(X, y)
    print selector.pvalues_

    # Get the raw p-values for each feature, and transform from p-values into scores
    scores = -np.log10(selector.pvalues_)
    print scores

    predictors = list(X.columns)
    plt.bar(range(len(predictors)), scores)
    plt.xticks(range(len(predictors)), predictors, rotation='vertical')
    plt.show()
    print selector.pvalues_,selector.pvalues_<= 0.05
    
   (4) Model fitting 
       Logistic Regression: model1 = LogisticRegression(random_state=0)
                            model1.fit(X_train, y_train)
   
       score = model1.score(X_train, y_train)
       print("Train Score: %0.3f" % score)

       score = model1.score(X_test, y_test)
       print("Test Score: %0.3f" % score)
       
       Random Forest: model2 = RandomForestClassifier(bootstrap=True, random_state=0, n_estimators=30, min_samples_split=3, #min_samples_leaf = 500,
                                oob_score=True, class_weight={0:1,1:ratio} #
                                #class_weight = 'balanced'
                               )
                      model2.fit(X_train, y_train)
       score = model2.score(X_train, y_train)
       print("Train Score: %0.3f" % score)

       score = model2.score(X_test, y_test)
       print("Test Score: %0.3f" % score)   
       "Out-of-bag score: " --> model2.oob_score_
   
   (5) Model Validation   
       Confusion Matrix: 
          from sklearn.metrics import confusion_matrix

          y_pred = model.predict(X_test)
          cm = confusion_matrix(y_test, y_pred)

          index = ['Non-HighSpender', 'HighSpender']
          columns = ['predicted_Non-HighSpender', 'predicted_HighSpender']

          pd.DataFrame(cm, index=index, columns=columns)
       + Scores: classification_report(y_test, y_pred)
       
       ROC AUC Curve:
        # Determine the false positive and true positive rates
        fpr, tpr, _ = roc_curve(y_test, model.predict_proba(X_test)[:,1])

        # Calculate the AUC
        roc_auc = auc(fpr, tpr)
        print 'ROC AUC: %0.2f' % roc_auc

        # Plot of a ROC curve for a specific class
        plt.figure()
        plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)
        plt.plot([0, 1], [0, 1], 'k--')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('ROC Curve')
        plt.legend(loc="lower right")
        plt.show()
        
      Adjust threshold:
        predictions = model.predict_proba(X_test)
        output= predictions >= 0.45
        output = output.astype(int)
        print confusion_matrix(y_test,output[:,1])
        
        print(classification_report(y_test, output[:,1]))
        
      Cross Validation
        cv_scores = CV.cross_val_score(model, X, y,cv=10)
        cv_predictions = CV.cross_val_predict(model5,X,y,cv=10)

        print metrics.accuracy_score(y, cv_predictions)

       #Precision and recall for expected outcomes
        print metrics.classification_report(y,cv_predictions)
       
        print("CV score: {:0.3f} +/- {:0.3f}".format(cvscores.mean(), cvscores.std()))
       
       #Leave One Out: 
        scores = cross_val_score(model, X, y, cv=LeaveOneOut(len(X)))
        scores.mean()
   
   (6) Variable Importance    
        from tabulate import tabulate
        headers = ["name", "score"]
        values = sorted(zip(X_train.columns, model2.feature_importances_), key=lambda x: x[1] * -1)
        print(tabulate(values, headers, tablefmt="plain"))
    
   (7) Grid Search
      from sklearn.model_selection import GridSearchCV


      param_grid = {'n_estimators': [30,40,50,80,100],
                    'min_samples_split': [2,3,4,5,10]}

      #grid = GridSearchCV(PolynomialRegression(), param_grid, cv=7)
      gsmodel = GridSearchCV(model2, param_grid, n_jobs=-1)
      gsmodel.fit(X_train, y_train)
      print("Best Model:")
      print(gsmodel.best_estimator_)
      print()
      print("Best Parameters:")
      print(gsmodel.best_params_)
      print()
      print("Best Score:")
      print(gsmodel.best_score_)
      
   (8) Prediction
      #Prediction data set
       X_predict_handles = model_predict_df.copy()
       proba_array_handles = model2.predict_proba(X_predict_handles)
       cols = ['<$500 Spend Probability', '>=$500 Spend Probability']
       proba_handles_df = pd.DataFrame(proba_array_handles,columns=cols)



## Plotting


## Sqlite
    import sqlite3

    conn = sqlite3.connect("mydatabase.db")
    cursor = conn.cursor()

    sql = """
    DELETE FROM albums
    WHERE artist = 'John Doe'
    """
    cursor.execute(sql)
    conn.commit()
